### 隐藏节点 

隐藏节点也持有主节点的数据副本，但是它对于应用是不可见的。那些工作负载和其他成员有所不同的节点(比如专门用于数据分析的节点)适合设置成为隐藏节点。隐藏节点必须同时priority设置为0，保证它不会成为主节点。db.isMaster() 不会显示隐藏节点。但是，隐藏节点可以具有投票权。

在下面5成员的副本集中，有四个从节点，但是有一个节点是隐藏的。
In the following five-member replica set, all four secondary members have copies of the primary’s data set, but one of the secondary members is hidden.

![avatar](https://docs.mongodb.com/manual/_images/replica-set-hidden-member.bakedsvg.svg)

+ **特征**
    * *读取操作*
        应用不会将读取操作分发到隐藏成员，因此，隐藏节点除了基本的数据复制之外，不和其他成员通信。可以使用隐藏节点做一些特定的任务，比如备份或者分析。延迟节点也需要是隐藏的。
        
        在一个分片集群中，mongos进程不会和隐藏节点通信。

    * **投票**    
    隐藏节点也具有投票的能力。如果你要停止一个隐藏节点，那么你需要保证集群具有足够的投票节点否则主节点会挂。

    
    For the purposes of backups,db.fsyncLock() ensures that the data files are safe to copy using low-level backup utilities such as cp, scp, or tar. A mongod started using the copied files contains user-written data that is indistinguishable from the user-written data on the locked mongod.

+ The data files of a locked mongod may change due to operations such as journaling syncs or WiredTiger snapshots. While this has no affect on the logical data (e.g. data accessed by clients), some backup utilities may detect these changes and emit warnings or fail with errors. For more information on MongoDB- recommended backup utilities and procedures, see MongoDB Backup Methods.

+ **Write Concern**
Hidden replica set members can acknowledge write operations issued with w: <number>. For write operations isued with w : "majority", however, hidden members must also be voting members (i.e. members[n].votes greater than 0) to acknowledge the "majority" write operation. Non-voting replica set members (i.e. members[n].votes is 0) cannot contribute to acknowledging write operations with majority write concern.

+ **Further Reading**
For more information about backing up MongoDB databases, see MongoDB Backup Methods. To configure a hidden member, see Configure a Hidden Replica Set Member.

### Delayed Replica Set Members

Delayed members contain copies of a replica set’s data set. However, a delayed member’s data set reflects an earlier, or delayed, state of the set. For example, if the current time is 09:52 and a member has a delay of an hour, the delayed member has no operation more recent than 08:52.

Because delayed members are a “rolling backup” or a running “historical” snapshot of the data set, they may help you recover from various kinds of human error. For example, a delayed member can make it possible to recover from unsuccessful application upgrades and operator errors including dropped databases and collections.

+ **Considerations**
    * *Requirements* 
        Delayed members:
        - Must be priority 0 members. Set the priority to 0 to prevent a delayed member from becoming primary.
        - Should be hidden members. Always prevent applications from seeing and querying delayed members.
        - do vote in elections for primary, if members[n].votes is set to 1.

+ **Behavior**
Delayed members copy and apply operations from the source oplog on a delay. When choosing the amount of delay, consider that the amount of delay:

+ must be equal to or greater than your expected maintenance window durations.
+ must be smaller than the capacity of the oplog. For more information on oplog size, see Oplog Size.

+ **Write Concern**
Delayed replica set members can acknowledge write operations issued with w: <number>. For write operations isued with w : "majority", however, delayed members must also be voting members (i.e. members[n].votes greater than 0) to acknowledge the "majority" write operation. Non-voting replica set members (i.e. members[n].votes is 0) cannot contribute to acknowledging write operations with majority write concern.

Delayed secondaries can return write acknowledgment no earlier than the configured slaveDelay.

+ **Sharding**
In sharded clusters, delayed members have limited utility when the balancer is enabled. Because delayed members replicate chunk migrations with a delay, the state of delayed members in a sharded cluster are not useful for recovering to a previous state of the sharded cluster if any migrations occur during the delay window.

+ **Example**
In the following 5-member replica set, the primary and all secondaries have copies of the data set. One member applies operations with a delay of 3600 seconds (one hour). This delayed member is also hidden and is a priority 0 member.

！[avatar](https://docs.mongodb.com/manual/_images/replica-set-delayed-member.bakedsvg.svg)

+ **Configuration**
A delayed member has its members[n].priority equal to 0, members[n].hidden equal to true, and its members[n].slaveDelay equal to the number of seconds of delay:
```
{
   "_id" : <num>,
   "host" : <hostname:port>,
   "priority" : 0,
   "slaveDelay" : <seconds>,
   "hidden" : true
}
```


## Replica Set Arbiter
In some circumstances (such as you have a primary and a secondary but cost constraints prohibit adding another secondary), you may choose to add an arbiter to your replica set. An arbiter does not have a copy of data set and cannot become a primary. However, an arbiter participates in elections for primary. An arbiter has exactly 1 election vote.

Changed in version 3.6: Starting in MongoDB 3.6, arbiters have priority 0. When you upgrade a replica set to MongoDB 3.6, if the existing configuration has an arbiter with priority 1, MongoDB 3.6 reconfigures the arbiter to have priority 0.

```
IMPORTANT
Do not run an arbiter on systems that also host the primary or the secondary members of the replica set.
```

+ **Example**
For example, in the following replica set with a 2 data bearing members (the primary and a secondary), an arbiter allows the set to have an odd number of votes to break a tie:

![avatar](https://docs.mongodb.com/manual/_images/replica-set-primary-with-secondary-and-arbiter.bakedsvg.svg)

+ **Read Concern majority and Three-Member PSA**
FOR 3-MEMBER PRIMARY-SECONDARY-ARBITER ARCHITECTURE*

If you have a three-member replica set with a primary-secondary-arbiter (PSA) architecture or a sharded cluster with a three-member PSA shards, the cache pressure will increase if any data bearing node is down and support for "majority" read concern is enabled.

To prevent the storage cache pressure from immobilizing a deployment with a three-member primary-secondary-arbiter (PSA) architecture, you can disable read concern “majority” starting in MongoDB 4.0.3 (and 3.6.1+). For more information, see Disable Read Concern Majority.

+ **Replica Set Protocol Version and Arbiter**
For the following MongoDB versions, pv1 increases the likelihood of w:1 rollbacks compared to pv0 (no longer supported in MongoDB 4.0+) for replica sets with arbiters:

+ MongoDB 3.4.1
+ MongoDB 3.4.0
+ MongoDB 3.2.11 or earlier

+ **Security**
    * *Authentication* 
        When running with authorization, arbiters exchange credentials with other members of the set to authenticate. MongoDB encrypts the authentication process, and the MongoDB authentication exchange is cryptographically secure.

        Because arbiters do not store data, they do not possess the internal table of user and role mappings used for authentication. Thus, the only way to log on to an arbiter with authorization active is to use the localhost exception.
    * *Communication*
        The only communication between <a href=""></a>rbiters and other set members are: votes during elections, heartbeats, and configuration data. These exchanges are not encrypted.

        However, if your MongoDB deployment uses TLS/SSL, MongoDB will encrypt all communication between replica set members. See Configure mongod and mongos for TLS/SSL for more information.

        As with all MongoDB components, run arbiters in trusted network environments.
